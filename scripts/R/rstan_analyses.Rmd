---
title: "bayesParcel R/Stan analyses"
author: "The Best Group Ever (Paul, Yaniv, Monica)"
date: "11/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(tidyverse)
theme_set(theme_bw())
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
library(data.table)
library(moments)
```
# Introduction

Functional magnetic resonance imaging (fMRI) is among the most popular methods used by psychologists and cognitive neuroscience to examine human brain activity. Because brain images for fMRI analysis contain hundreds of thousands of voxels (3-dimensional pixels) multiple comparisons is a large issue when attempting to make inference using this type of data. Over the last several years, the MRI field in general has become more aware of these issues as the power and replicability of many MRI studies has been shown to often be quite low [(Geuter et al., 2018](https://www.biorxiv.org/content/early/2018/04/06/295048); [Button et al., 2013](https://www.nature.com/articles/nrn3475); [Lieberman & Cunningham, 2009](https://academic.oup.com/scan/article/4/4/423/1670802)).

One reason in particular why MRI analyses can tend to be low-powered is that voxel-level data is extremely noisy and unreliable. When conducting analyses to make inferences about populations, researchers typically first estimate subject-level activity at each voxel (we do not focus on these subject-level estimates for the purposes of this analysis), then when making inference on groups of subjects, use strict false discovery rate (FDR) controlling procedures to control Type 1 errors for which individual voxels are active under a null hypothesis significance testing (NHST) framework [(Genovese, Lazar, & Nichols, 2002)](https://www.sciencedirect.com/science/article/pii/S1053811901910377). This method of running a FDR-corrected test at the population level on each voxel independently, known as the 'Massively Univarite' approach to fMRI analysis, is currently an extremely popular tool. 

Alternatively, if one has a prior hypothesis about a specific brain region for a population-level effect, it is common to use the 'region of interest' (ROI) approach, where population inference is done after voxelwise estimates are averaged across a spatial region of voxels. While this approach make assumptions about voxels within an ROI serving as one functional unit, because only one region is being investigated, power to detect true effects is, at least theoretically, higher. In practice, however, researchers may have many degrees of freedom in selecting ROIs, and estimates are biased if labs make it a practice of searching many regions (effectively making many comparisons) using NHST. 

Inspired by ideas from a recent preprint [(Chen et al., 2018)](https://www.biorxiv.org/content/biorxiv/early/2017/12/22/238998.full.pdf) we set out here to try to offer an alternate approach to whole-brain inference by employing Bayesian Hierarchical Modeling with ROIs as the observational units. This approach should let us estimate activity for all regions at the population level simultaneously, using partial pooling to effectively account for multiple comparisons [(Gelman, Hill, & Yajima, 2012)](https://www.tandfonline.com/doi/abs/10.1080/19345747.2011.618213). Under this approach, we model the predicted activity for each region, within each subject. Similar to the 8 schools example [(Gelman et al., 2013)](https://www.taylorfrancis.com/books/9781439898208), we have data in the form of an estimate and standard deviation for each ROI, for each subject. ROIs in this case are defined using 68 regions of structural/functional signifance from the [Harvard-Oxford Brain Atlas](https://scalablebrainatlas.incf.org/human/HOA06s)


Crucial to our models are the hyperparameters for the distributions from which different regions and subjects are drawn from. To address this, we try two models: 

  * Model 0: This model assumes that all ROIs, at the population level, are drawn from one common distribution with a prior centered around 0 (or no activity). 
  * Model 1: This model attempts to classify ROIs into mixture components of 'positively active', 'inactive', and 'negatively active' rather than assuming ROIs area all drawn from a common distribution. 
  
Below, we demonstrate both models, and address their efficacy on both simulated and real data. 


## Loading real data

For this example, we test our model on an emotional face categorization task scanned using fMRI on a group of healthy devloping children. In the task, participants are shown different emotional faces and asked to press buttons to categorize which emotion they are seeing (neutral versus fearful faces). For our purposes, we focuse on estimating the activity in the brain associated with viewing the fearful faces above the 'baseline' of a fixation cross on the screen. For more information on this task, see [(Gee et al., 2013)](https://www.ncbi.nlm.nih.gov/pubmed/23467374)

Similarly to the 8 schools example, we have a signal estimate for fearful faces for each ROI for each subject. In addition, we have a standard deviation for each of these estimates. Our data are formatted as below:

```{r}
betas_raw <- read_csv("../../ignore/harvard_ox_roi.csv")

betas <- betas_raw %>%
  gather(key = "roi", value = "value", -Subject, -wave, -meanFD_included_trs) %>%
  filter(!is.na(value)) %>%
  mutate(statistic = if_else(grepl("Mean", roi), "cope_mean", "cope_sd"),
         roi = if_else(statistic == "cope_mean",
                       stringr::str_sub(roi, end = -5L),
                       stringr::str_sub(roi, end = -3L))) %>%
  spread(statistic, value) %>%
  # Filter out cope sd's that are 0 -- this would indicate an error in preprocessing
  filter(cope_sd > 0, wave == 1) %>%
  group_by(Subject) %>%
  nest() %>%
  mutate(subject_num = 1:n()) %>%
  unnest() %>%
  group_by(roi) %>%
  nest() %>%
  mutate(roi_num = 1:n()) %>%
  unnest() %>%
  mutate(cope_mean_scaled = cope_mean / sd(cope_mean),
         cope_sd_scaled = cope_sd / sd(cope_mean))

head(betas)
```

```{r}
# Exclude nonsense ROIs
exclude <- c('harvardox_subcortical_1', 'harvardox_subcortical_2', 'harvardox_subcortical_8',
             'harvardox_subcortical_12', 'harvardox_subcortical_13')
betas <- subset(betas, !(roi %in% exclude))
```

# Model 0: Single normal hyper-distribution
As a starting point, we chose to model COPE responses in the different ROIs with a model based on the 8-school example. As such, this is a hierarchical model, with each ROI drawn from a population of ROIs with a normal distribution, and each subject drawn from a population of subjects with a normal distribution. Thus, the likelihood in our model is:
$$COPE_{i} \sim normal(\alpha + \tau_{ROI} * \eta_{ROI[i]} + \tau_{subject} * \eta_{subject[i]}) $$
With $\alpha$ being a general intercept term, $\eta$s as the ROI- and subject-wise deviations from the general intercept, and $\tau$s as the variation factor for the deviations.
Following the 8-school model, we chose normal mid-level priors for ROIs and subjects:
$$\eta_{subject} \sim normal(0,1)$$
$$\eta_{ROI} \sim normal(0,1)$$
Finally, we used non-informative priors for the variance and general expected value hyper parameters ($\alpha$ and $\tau_{subject / ROI}$), for regularization purposes.

## Stan code
```{stan, output.var = "model0", results = "hide", message = FALSE, warning = FALSE}
data {
  int<lower=0> N;          // number of rois x observations (subjs x waves)
  int<lower=0> N_roi;
  int<lower=0> N_subj;
  real cope[N];               // estimated effects for each observation of roi
  int<lower=1, upper=N_roi> roi[N]; // ROI ID of each observation
  int<lower=1, upper=N_subj> subj[N]; // subject ID of each observation
  real<lower=0> fd[N]; // framewise displacement of each observation, varies by subject x wave
  real<lower=0> varcope[N];  // s.e. of effect estimates
}
transformed data {
  int<lower=0> N_cell = N_roi * N_subj;
}
parameters {
  real alpha; // intercept 
  real<lower=0> tau_roi;
  real<lower=0> tau_subj;
  vector[N_roi] eta_roi; // centered parameterization of roi-specific effect estimate
  vector[N_subj] eta_subj; // centered parameterization of roi-specific effect estimate
}
model {
  // priors make the world go round
  alpha ~ normal(0, 1);
  tau_roi ~ normal(0, 1);
  tau_subj ~ normal(0, 1);
  eta_roi ~ normal(0, 1);
  eta_subj ~ normal(0, 1);
  // completely pooled across observations here
  cope ~ normal(alpha + tau_roi * eta_roi[roi] + tau_subj * eta_subj[subj], varcope);
}
generated quantities {
  vector[N_roi] theta_roi = alpha + tau_roi * eta_roi;
  vector[N_subj] theta_subj = alpha + tau_subj * eta_subj;
  
  vector [N] rep_cope; // Create data replicates

  for (i in 1:N)
    rep_cope[i] = normal_rng(alpha + tau_roi * eta_roi[roi][i] + tau_subj * eta_subj[subj][i], varcope[i]);
}
```
## Fake data simulation
Before fitting the model to the data, we'll begin by evaluating the model's potency in recovering known parameters. We used Stan to simulate data given parameters drawn from the priors in our model. We then fit the simulated data, and checked the resultant posterior intervals against the know values of the parameters. 
```{stan, output.var = "model0_fake", results = "hide", message = FALSE, warning = FALSE}
data {
  int<lower=0> N;          // number of rois x observations (subjs x waves)
  int<lower=0> N_roi;
  int<lower=0> N_subj;
  int<lower=1, upper=N_roi> roi[N]; // ROI ID of each observation
  int<lower=1, upper=N_subj> subj[N]; // subject ID of each observation
  real<lower=0> varcope[N];  // s.e. of effect estimates
}
parameters {
  real alpha; // intercept 
  real<lower=0> tau_roi;
  real<lower=0> tau_subj;
  vector[N_roi] eta_roi; // centered parameterization of roi-specific effect estimate
  vector[N_subj] eta_subj; // centered parameterization of roi-specific effect estimate
}
model {
  // priors make the world go round
  alpha ~ normal(0, 1);
  tau_roi ~ normal(0, 1);
  tau_subj ~ normal(0, 1);
  eta_roi ~ normal(0, 1);
  eta_subj ~ normal(0, 1);
}
generated quantities {
  vector [N] cope; // Create data replicates
  for (i in 1:N)
    cope[i] = normal_rng(alpha + tau_roi * eta_roi[roi][i] + tau_subj * eta_subj[subj][i], varcope[i]);
}
```

```{r results = "hide", message = FALSE, warning = FALSE}
# Draw fake data
fake_data0 <- sampling(model0_fake, data = betas %$%
                   list(N = nrow(.),
                        N_roi = max(roi_num),
                        N_subj = max(subject_num),
                        roi = roi_num,
                        subj = subject_num,
                        fd = meanFD_included_trs,
                        varcope = cope_sd_scaled),
                   iter = 1, warmup = 0, chain = 1, seed = 78)

# Fake data book keeping
fake_data0 <- as.data.table(fake_data0)
fake_params0 <- fake_data0[, !grepl('cope', colnames(fake_data0)), with = F]
fake_data0 <- fake_data0[, grepl('cope', colnames(fake_data0)), with = F]
fake_data0 <- melt(fake_data0, measure.vars = colnames(fake_data0))
```

``` {r eval = F}
# Fit fake data
fake_fit0 <- sampling(model0,
                 data = betas %$%
                   list(N = nrow(.),
                        N_roi = max(roi_num),
                        N_subj = max(subject_num),
                        cope = fake_data0$value,
                        roi = roi_num,
                        subj = subject_num,
                        fd = meanFD_included_trs,
                        varcope = cope_sd_scaled),
                 seed = 2323)
save(fake_fit0, file = "../../ignore/fake_fit0.rda")
```

```{r echo = F}
load("../../ignore/fake_fit0.rda")
```

```{r}
# Check parameter recovery
fake_fit0 <- as.data.table(fake_fit0)
fake_fit0 <- melt(fake_fit0, measure.vars = colnames(fake_fit0)) 
fake_fit0 <- fake_fit0[, .(median = median(value),
                           ub = quantile(value, 0.975),
                           lb = quantile(value, 0.025)), by = variable]
fake_params0 <- melt(fake_params0, measure.vars = colnames(fake_params0), value.name = 'trueVal')
fake_fit0 <- merge(fake_fit0, fake_params0, by = 'variable')
fake_fit0 <- fake_fit0[!grepl('lp_', variable, fixed = T),]

ggplot(fake_fit0[grepl('eta_roi',variable),], aes(x = median, y = variable)) +
  geom_errorbarh(aes(xmin = lb, xmax = ub)) +
  geom_point() +
  geom_point(aes(x = trueVal), color = 'red') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  labs(x = 'Parameter value',
       title = 'Simulated data parameter recover: eta_ROI',
       subtitle = 'Black: posterior median and 95% interval. Red: true parameter value')

ggplot(fake_fit0[grepl('eta_subj',variable),], aes(x = median, y = variable)) +
  geom_errorbarh(aes(xmin = lb, xmax = ub)) +
  geom_point() +
  geom_point(aes(x = trueVal), color = 'red') +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  labs(x = 'Parameter value',
       title = 'Simulated data parameter recover: eta_subject',
       subtitle = 'Black: posterior median and 95% interval. Red: true parameter value')

ggplot(fake_fit0[!grepl('eta_roi',variable) &!grepl('eta_subj',variable),], 
       aes(x = median, y = variable)) +
  geom_errorbarh(aes(xmin = lb, xmax = ub)) +
  geom_point() +
  geom_point(aes(x = trueVal), color = 'red') +
  labs(x = 'Parameter value',
       title = 'Simulated data parameter recover: hyperparameters',
       subtitle = 'Black: posterior median and 95% interval. Red: true parameter value')
```
As can be seen in the plots above, for the vast majority of parameters in this fit, the true value lies within the 95% posterior interval, as we would expect. Since we drew parameters from the priors for our model, we can see this result as rather general.

## Model fit to data
Next, we fit the observed data with our model.
```{r, eval = FALSE}
fit0 <- sampling(model0,
                 data = betas %$%
                   list(N = nrow(.),
                        N_roi = max(roi_num),
                        N_subj = max(subject_num),
                        cope = cope_mean_scaled,
                        roi = roi_num,
                        subj = subject_num,
                        fd = meanFD_included_trs,
                        varcope = cope_sd_scaled),
                 control = list(max_treedepth = 15),
                 seed = 909)

save(fit0, file = "../../ignore/fit_model0.rda")
```

```{r echo = F}
load("../../ignore/fit_model0.rda")
```

```{r}
df_fit0 <- fit0 %>%
  as.data.frame() %>%
  as_tibble() %>%
  mutate(iteration = 1:n())

hyperparams <- df_fit0 %>%
  select(iteration, alpha, tau_roi, tau_subj)

theta_rois <- df_fit0 %>%
  select(iteration, starts_with("theta_roi")) %>%
  gather(key = "roi_num", value = "theta", starts_with("theta_roi")) %>%
  mutate(roi_num = as.integer(str_sub(roi_num, start = 11L, end = -2L)),
         theta_unscaled = theta * sd(betas$cope_mean)) %>%
  group_by(roi_num) %>%
  nest(.key = "iterations") %>%
  left_join(betas %>%
              select(roi_num, cope_mean, cope_mean_scaled) %>%
              group_by(roi_num) %>%
              summarize_all(mean),
            by = "roi_num") %>%
  mutate(summaries = map(iterations, ~.x %>%
                           summarize_at(vars(starts_with("theta")),
                                        funs(median = median,
                                             int_95_lower = quantile(., .025),
                                             int_95_upper = quantile(., .975),
                                             int_50_lower = quantile(., .25),
                                             int_50_upper = quantile(., .75))))) %>%
  unnest(summaries, .preserve = "iterations")


```

After fitting the model, we proceeded to perform some model check and evaluations. Comparing the observed COPE values to the estimated posterior intervals, we find that the estimated intervals are pooled towards the sample mean, in comparison to the raw values. The plot below illustrates this point. This is an expected advantageous feature of the hierarchical model, which allows partial pooling of estimates across ROIs and subjects.
```{r}
theta_rois %>%
  ggplot(aes(x = roi_num)) +
  geom_hline(yintercept = 0, linetype = 3) +
  geom_errorbar(aes(ymin = theta_int_95_lower, ymax = theta_int_95_upper), width = 0) +
  geom_point(aes(y = theta_median)) +
  geom_point(aes(y = cope_mean_scaled), color = "hotpink") +
  labs(x = "ROI number (arbitrary)",
       y = "COPE/beta value (SCALED arbitrary units)",
       title = "Bayesian estimates against original marginal means",
       subtitle = "Black: median estimates +- 95% predictive interval, pink: original mean estimate") +
  theme_bw()
```

### Brain plots

### Posterior predictive checks
To further evaluate the adequacy of the hierarchical normal model to the data, we performed posterior predictive checks. For each posterior draw, simulated data were generated with the same model likelihood, given the drawn parameters. These posterior replicates allow us to plot the interval of expected new observations, given our model and the observed data. Comparing our observed data to the predictive interval informs us about the adequacy of the chosen model.
As can be seen of in the plots of replicates and observed data below, the observed data all fall withing the 95% interval of replicates. Thus, the data are plausible given the generative model. A drawback highlighted by this test, is that the median replicates are do not follow the observed data well at the extreme tails of the distribution: that is, the observed data seems to have consistently heavier tails than than the replicate data.
```{r}
# Plot data replicates
# Implemented with data.table. Sorry. Open to the possibility of recoding with tidyverse, just not right now
df_fit0 <- as.data.table(fit0)
df_fit0 <- df_fit0[, grepl('rep_cope', colnames(df_fit0)), with = F]
df_fit0[, sim := 1:nrow(df_fit0)]
df_fit0 <- melt(df_fit0, id.vars = 'sim')
df_fit0[, value := value * sd(betas$cope_mean)]
df_fit0 <- df_fit0[, .(median = median(value),
                     lb = quantile(value, 0.025),
                     ub = quantile(value, 0.975)), by = variable][order(variable)]
df_fit0[, subject_num := betas$subject_num]
df_fit0[, roi_num := betas$roi_num]
df_fit0[, true_value := betas$cope_mean]
df_fit0 <- df_fit0[order(true_value)]

# Plot disregarding heirarchy
ggplot(df_fit0, aes(x = 1:nrow(df_fit0), y = median)) +
  geom_ribbon(aes(ymin = lb, ymax = ub), fill = 'yellow') +
  geom_line(color = 'darkred') + 
  geom_point(aes(y = true_value)) +
  labs(x = 'Observation number',
       y = 'COPE',
       title = 'Posterior predictive checks: replicate plots ignoring heirarchy',
       subtitle = 'Red: posterior median. Yellow: posterior 95% interval. Black: observed COPE')

# Plot by subject
df_subs <- df_fit0[subject_num %in% sample(1:length(unique(subject_num)), 20)]
df_subs[, xplot := 1:.N, by = subject_num]
ggplot(df_subs, aes(x = xplot, y = median)) +
  geom_ribbon(aes(ymin = lb, ymax = ub), fill = 'yellow') +
  geom_point(aes(y = true_value), size = 0.3) +
  geom_line(color = 'darkred') + 
  facet_wrap('subject_num') +
  theme(strip.background = element_blank(),
    strip.text.x = element_blank()) +
  labs(x = 'Subject number (arbitrary)',
       y = 'COPE',
       title = 'Posterior predictive checks: replicate plots by subject, for a random subsample of subjects',
       subtitle = 'Red: posterior median. Yellow: posterior 95% interval. Black: observed COPE')

# Plot by roi
df_subs <- df_fit0[roi_num %in% sample(1:length(unique(roi_num)), 20)]
df_subs[, xplot := 1:.N, by = roi_num]
ggplot(df_subs, aes(x = xplot, y = median)) +
  geom_ribbon(aes(ymin = lb, ymax = ub), fill = 'yellow') +
  geom_point(aes(y = true_value), size = 0.3) +
  geom_line(color = 'darkred') + 
  facet_wrap('roi_num') +
  theme(strip.background = element_blank(),
    strip.text.x = element_blank()) +
  labs(x = 'ROI number (arbitrary)',
       y = 'COPE',
       title = 'Posterior predictive checks: replicate plots by ROI, for a random subsample of ROIs',
       subtitle = 'Red: posterior median. Yellow: posterior 95% interval. Black: observed COPE')

```
The heavier tails in the observed data indicated that our mid-level priors might not be the best fit for the data. Hence, we compared the estimated draws for the ROI and subject deviations from the mean COPE to the normal prior we assigned them. The plots below reveal that overall, the estimated deviations conform to the shape of the normal prior. However, when inspecting the plots for the ROIs, they seem to have a rightward skew and a heavier right tail that is not consistent with a normal prior.
```{r}
# Compare thetas to priors
df_fit0 <- as.data.table(fit0)
vars <- colnames(df_fit0)
eta_rois <- df_fit0[sample(1:nrow(df_fit0), 20), grepl('eta_roi', vars) & !grepl('th', vars), with = F]
eta_subj <- df_fit0[sample(1:nrow(df_fit0), 20), grepl('eta_subj', vars) & !grepl('th', vars), with = F]

eta_rois[, sim := 1:.N]
eta_subj[, sim := 1:.N]

eta_rois <- melt(eta_rois, id.vars = 'sim')
eta_subj <- melt(eta_subj, id.vars = 'sim')

bw <- .2
n_subj <- length(unique(eta_subj$variable))
ggplot(eta_subj, aes(x = value)) + 
  geom_histogram(binwidth = bw) +
  facet_wrap('sim') +
  stat_function(fun = function(x, bw, n) dnorm(x) * bw * n, 
                args = c(bw = bw, n = n_subj),
                color = 'blue') +
  theme(strip.background = element_blank(),
    strip.text.x = element_blank()) + 
  labs(x = expression(eta),
       title = 'Estimated subject deviations agains the normal prior distribution',
       subtitle = 'Black: subject histogram, Blue: normal prior, Each facet represents a posterior draw')

n_roi <- length(unique(eta_rois$variable))
ggplot(eta_rois, aes(x = value)) + 
  geom_histogram(binwidth = bw) +
  facet_wrap('sim') +
  stat_function(fun = function(x, bw, n) dnorm(x) * bw * n, 
                args = c(bw = bw, n = n_roi),
                color = 'blue') +
  theme(strip.background = element_blank(),
    strip.text.x = element_blank()) + 
  labs(x = expression(eta),
       title = 'Estimated ROI deviations agains the normal prior distribution',
       subtitle = 'Black: ROI histogram, Blue: normal prior, Each facet represents a posterior draw')

```
### Model 0 summary
Fitting a hierarchical normal model to the data, with normally distributed deviations for ROIs and subjects, seems like a good first choice for the data. The estimates are stable and interpretable, with observed data plausible given the generative model. However, a  tendency towards rightward skew in the distribution of estimated ROIs, and the comparison of replicates and observed data, prompted us to suspect that the extreme observations could be better modeled. This is in line with the modal theoretical view of imaging experiments: It is expected that regions in the brain should come from separate distributions. Commonly, it is assumed that a group of regions is positively activated by a given task, another group does not show activation, and possibly a third group shows negative values for the contrast of interest.

# Model 1: Finite mixture hyper-distribution

We decided that we would implement this theoretically motivated second model as a finite mixture model. In this case, we would assume one mixture component for "non-activated" regions, one mixture component for "negative-activated" regions, and one for "positive-activated" regions. In this way, we would be able to classify regions as activated vs. not based on their estimated probabilities of belonging to each mixture component.

For our first pass of the model, we tried our best to implement it without tailoring too many of the model elements (standard deviations on normal priors, for example) to observed features of the data. This is a little different than the usual strategy of visually exploring data to determine how best to fit a model to said data. We opted to do this to model how this analysis method might be used "in the wild." With most neuroimaging data, data are collected to fit a prescribed data structure (MRI image timeseries should be a certain structure, event markers for different sections of the timeseries should be labeled a certain way, etc), and researchers use a fairly stereotyped analysis pipeline that is robust to individual dataset-level variations. If we were to generalize the model we present here to future analyses, it would need to be completely generative, and thus not have parameters specified based on the data fed into it. If the generative model fits less well to the current dataset, we prefer that if the model can be generalized to future datasets without modification.

We opted to fit a mixture model with three ordered normal components. We constrained the first component to have a negative mean, the second component to have a mean of 0, and the third component to have a positive mean. We set the standard deviations of each of the mixture components to be equal, as we did not have a strong prior reason to believe the mixture components to have different spreads, and thus we wanted to minimize the number of parameters to be estimated.

We specified the $\eta_{ROI}$ and $\eta_{subject}$ parameters similarly to the first, single normal model, but this time we specified $\eta_{ROI}$ as a matrix with $N$ rows, one for each ROI (as before), and with $K$ columns, one for each mixture component (new). Each ROI has one $\eta_{ROI_k}$ value, conditional on the ROI being drawn from mixture component $k$.

We added two new sets of parameters, $\beta$, for the means of each of the mixture components, and $\phi$, for the latent variable indexing the probability that an observation would be drawn from a given mixture component.

### Delete me simulation maybe

```{r}
tibble(id = 1:2000) %>%
  mutate(mu_mixture = sample(-1:1, size = n(), replace = TRUE, prob = c(.25, .5, .25)),
         mu_single = 0,
         sigma = 0.5) %>%
  gather(key = "type", value = "mu", starts_with("mu")) %>%
  mutate(x = rnorm(n(), mean = mu, sd = sigma)) %>%
  ggplot(aes(x = x, fill = type)) +
  geom_histogram(position = "identity", alpha = 0.2)
```


### Stan code

```{stan, output.var = "model1", results = "hide", message = FALSE, warning = FALSE}
data {
  real beta_0;                          // middle mixture center
  int<lower=0> N;                       // number of ROIs x observations (subjs x waves)
  int<lower=0> N_roi;                   // number of ROIs
  int<lower=0> N_subj;                  // number of subjects
  real cope[N];                         // estimated effects for each observation of roi
  int<lower=1, upper=N_roi> roi[N];     // ROI ID of each observation
  int<lower=1, upper=N_subj> subj[N];   // subject ID of each observation
  real<lower=0> varcope[N];             // s.e. of effect estimates
}
parameters {
  simplex[3] phi;           // mixing proportions
  real<upper=0> beta_neg;   // center of negative mixture component
  real<lower=0> beta_pos;   // center of positive mixture component
  real<lower=0> tau_roi;    // SD of hyper-distribution of eta_roi
  real<lower=0> tau_subj;   // SD of hyper-distribution of eta_subj
  matrix[N_roi, 3] eta_roi; // centered parameterization of roi-specific effect estimate
  vector[N_subj] eta_subj;  // centered parameterization of roi-specific effect estimate
}
transformed parameters {
  /* hard coding 3 mixture components
  kind of hacky, but this was the easiest way we figured out */
  vector[3] beta;
  beta[1] = beta_neg;
  beta[2] = beta_0;
  beta[3] = beta_pos;
}
model {
  vector[3] log_phi = log(phi);  // cache log calculation
  
  // weakly informative prior for the two estimated betas
  beta_neg ~ normal(0, 5);
  beta_pos ~ normal(0, 5);
  
  // wip for taus
  tau_roi ~ normal(0, 1);
  tau_subj ~ normal(0, 1);
  
  // wip for etas
  for (k in 1:3) {
    eta_roi[, k] ~ normal(0, 1);
  }
  
  eta_subj ~ normal(0, 1);
  
  // likelihood stuff
  for (n in 1:N) {
    vector[3] lps = log_phi;
    for (k in 1:3)
      lps[k] += normal_lpdf(cope[n] | beta[k] + tau_roi * eta_roi[roi[n], k] + tau_subj * eta_subj[subj[n]], varcope[n]);
    target += log_sum_exp(lps);
  }
}
generated quantities {
  // generated thetas, one for each mixture component
  matrix[N_roi, 3] theta_roi;
  for (k in 1:3) {
    theta_roi[, k] = beta[k] + tau_roi * eta_roi[, k];
  }
}

```

```{r, eval = FALSE}
fit1.2 <- sampling(model1.2,
                 data = betas %$%
                   list(K = 2L,
                        N = nrow(.),
                        N_roi = max(roi_num),
                        N_subj = max(subject_num),
                        cope = cope_mean_scaled,
                        roi = roi_num,
                        subj = subject_num,
                        fd = meanFD_included_trs,
                        varcope = cope_sd_scaled),
                 iter = 200,
                 control = list(max_treedepth = 15))
```

```{r, eval = FALSE}
fit1.3 <- sampling(model1.3,
                 data = betas %$%
                   list(K = 2L,
                        beta = c(0.3, 1),
                        N = nrow(.),
                        N_roi = max(roi_num),
                        N_subj = max(subject_num),
                        cope = cope_mean_scaled,
                        roi = roi_num,
                        subj = subject_num,
                        fd = meanFD_included_trs,
                        varcope = cope_sd_scaled),
                 iter = 200,
                 control = list(max_treedepth = 15))
```

Inspecting the estimates for $\phi$ and $\beta$ reveals that the mixture model does not actually add any new information. $\phi_3$, the probability for the third (positive) mixture component, is functionally 1, while $\phi_1$ and $\phi_2$ are functionally 0. $\beta_3$ is estimated at about the mean of the observed data, while the other two mixture centers are estimated at 0--$\beta_2$ because we fixed it at 0, and $\beta_1$ presumably because it's centered at the mean value of the prior, and there are no data points that would pull the posterior estimate below 0.


```{stan, output.var = "model1.2", results = "hide", message = F, warning = F, eval = F}
data {
  int<lower=1> K;                       // number of mixture components
  int<lower=0> N;                       // number of ROIs x observations (subjs x waves)
  int<lower=0> N_roi;                   // number of ROIs
  int<lower=0> N_subj;                  // number of subjects
  real cope[N];                         // estimated effects for each observation of roi
  int<lower=1, upper=N_roi> roi[N];     // ROI ID of each observation
  int<lower=1, upper=N_subj> subj[N];   // subject ID of each observation
  real<lower=0> varcope[N];             // s.e. of effect estimates
}
transformed data {
  int<lower=0> N_cell = N_roi * N_subj;
}
parameters {
  simplex[K] phi;           // mixing proportions
  ordered[K] beta;          // centers of mixture components
  real<lower=0> tau_roi;    // SD of hyper-distribution of eta_roi
  real<lower=0> tau_subj;   // SD of hyper-distribution of eta_subj
  matrix[N_roi, K] eta_roi; // centered parameterization of roi-specific effect estimate
  vector[N_subj] eta_subj;  // centered parameterization of roi-specific effect estimate
}
model {
  vector[2] log_phi = log(phi);  // cache log calculation
  
  // weakly informative prior for all betas
  beta ~ normal(0, 2);
  
  // weakly informative prior for taus
  tau_roi ~ normal(0, 1);
  tau_subj ~ normal(0, 1);
  
  // weakly informative prior for etas
  for (k in 1:K) {
    eta_roi[, k] ~ normal(0, 1);
  }
  
  eta_subj ~ normal(0, 1);
  
  // likelihood stuff
  for (n in 1:N) {
    vector[K] lps = log_phi;
    for (k in 1:K)
      lps[k] += normal_lpdf(cope[n] | beta[k] + tau_roi * eta_roi[roi[n], k] + tau_subj * eta_subj[subj[n]], varcope[n]);
    target += log_sum_exp(lps);
  }
}

```

```{stan, output.var = "model1.3", results = "hide", message = F, warning = F eval = F}
data {
  int<lower=1> K;                       // number of mixture components
  ordered[K] beta;                      // centers of mixture components
  int<lower=0> N;                       // number of ROIs x observations (subjs x waves)
  int<lower=0> N_roi;                   // number of ROIs
  int<lower=0> N_subj;                  // number of subjects
  real cope[N];                         // estimated effects for each observation of roi
  int<lower=1, upper=N_roi> roi[N];     // ROI ID of each observation
  int<lower=1, upper=N_subj> subj[N];   // subject ID of each observation
  real<lower=0> varcope[N];             // s.e. of effect estimates
}
parameters {
  simplex[K] phi;           // mixing proportions
  real<lower=0> tau_roi;    // SD of hyper-distribution of eta_roi
  real<lower=0> tau_subj;   // SD of hyper-distribution of eta_subj
  matrix[N_roi, K] eta_roi; // centered parameterization of roi-specific effect estimate
  vector[N_subj] eta_subj;  // centered parameterization of roi-specific effect estimate
}
model {
  vector[2] log_phi = log(phi);  // cache log calculation
  
  // rather informative prior for all betas
  beta ~ normal(0, 1);
  
  // weakly informative prior for taus
  tau_roi ~ normal(0, 1);
  tau_subj ~ normal(0, 1);
  
  // weakly informative prior for etas
  for (k in 1:K) {
    eta_roi[, k] ~ normal(0, 1);
  }
  
  eta_subj ~ normal(0, 1);
  
  // likelihood stuff
  for (n in 1:N) {
    vector[K] lps = log_phi;
    for (k in 1:K)
      lps[k] += normal_lpdf(cope[n] | beta[k] + tau_roi * eta_roi[roi[n], k] + tau_subj * eta_subj[subj[n]], varcope[n]);
    target += log_sum_exp(lps);
  }
}

```

## Attempts to refine model

We opted not to spend time running posterior predictive checks or other model evaluation on the first iteration of our mixture model, as we could tell immediately from checking the mixture parameter estimates that Stan had estimated our model as having essentially only one component.

We tried modifying the mixture model to remedy the non-mixture-estimation issue in the following ways. Stan code printed for transparency, but model outputs hidden for clarity, as we found that all of the modified models continued suffering from the problem of having one mixture component with $\phi \approx 1$ and the rest with $\phi \approx 0$.

First, we cut the number of mixture components down to two, as we observed in the original single normal hierarchical model that there were hardly any ROIs for which $\eta_{ROI}$ was "deactivated", or much below 0. This way, we should only be estimating the "non-activated" and the "positive-activated" mixture components. We also stopped hard-coding the number of components, or the location of any of the components, in this model.


### Plots?

Temporarily need to reload model object when done separately
```{r, echo = FALSE, eval = FALSE}
load('fit_model0.rda')

## Redundant bit
# 
# df_fit0 <- fit %>%
#   as.data.frame() %>%
#   as_tibble() %>%
#   mutate(iteration = 1:n())
# 
# theta_rois <- df_fit0 %>%
#   select(iteration, starts_with("theta_roi")) %>%
#   gather(key = "roi_num", value = "theta", starts_with("theta_roi")) %>%
#   mutate(roi_num = as.integer(stringr::str_sub(roi_num, start = 11L, end = -2L)),
#          theta_unscaled = theta * sd(betas$cope_mean)) %>%
#   group_by(roi_num) %>%
#   nest(.key = "iterations") %>%
#   left_join(betas %>%
#               select(roi_num, cope_mean, cope_mean_scaled) %>%
#               group_by(roi_num) %>%
#               summarize_all(mean),
#             by = "roi_num") %>%
#   mutate(summaries = map(iterations, ~.x %>%
#                            summarize_at(vars(starts_with("theta")),
#                                         funs(median = median,
#                                              int_95_lower = quantile(., .025),
#                                              int_95_upper = quantile(., .975),
#                                              int_50_lower = quantile(., .25),
#                                              int_50_upper = quantile(., .75))))) %>%
#   unnest(summaries, .preserve = "iterations") 

# Save out the thetas for each roi
theta_rois = dplyr::select(betas, roi, roi_num) %>%
  unique() %>%
  left_join(theta_rois, .) %>%
  select(-iterations)

write.csv(theta_rois, file = '../../estimates/spreadsheets/mod0.csv', row.names = F, quote = F)
```

# Discussion

## Why we trashed the mixture models
```{r}
betas %>%
  ggplot(aes(x = roi_num, y = cope_mean_scaled)) +
  geom_errorbar(aes(ymin = cope_mean_scaled - cope_sd_scaled, ymax = cope_mean_scaled + cope_sd_scaled), width = 0, alpha = 0.3) +
  geom_point(alpha = 0.3)

ggplot(betas, aes(x = cope_mean_scaled)) +
  geom_histogram(binwidth=.2)

summary(betas)
```

## Future directions

While rudimentary, our Bayesian multilevel modeling approach to estimating whole-brain activity at the level of ROIs seem promising both with simulated and real data. We believe that this approach may offer an alternative that is more efficient and robust to type M/type S errors when estimating patterns of brain activity. However, several important considerations for application and extension of such models remain. 

### Voxelwise Spatial Autocorelation


The mass univariate approach assumes to a degree that all voxels are independent units, but physiological patterns in the brain are known to be spatially autocorrelated. In addition, to decrease voxel-wise noise and increase power, it is standard in many fMRI analyses to employ 'spatial smoothing' across voxels; thus introducing even more spatial autocorrelation [(Worsley, 2005)](https://www.sciencedirect.com/science/article/pii/S1053811905000923). Although the ROI approach we have taken averages across spatial units comprised of many (hundreds or thousands) of voxels, as we plan to extend our model an additional level down to partially pool across all voxels, consideration of this spatial autocorrelation will be crucial. Indeed, even in our calculation of ROI-wise standard deviations, we have ignored this autocorrelation which may bias estimates. Future extensions of these models could make use of several methods for estimating and incorporating spatial autocorrelation, such as CAR models, or perhaps more simply, AR1 priors based on simple distance for voxelwise autocorrelation. 

### Repeated-Measures fMRI Data

Many fMRI studies examine brain activity in individuals across multiple scanning sessions. Our current model just estimates average activity across a sample of individuals assuming ever subject was scanned once, but we plan to adapt the model to accomodate repeated measurements within subjects to be able to estimate effects of interventions or longitudinal change. This would mean adding an additional hierarchical grouping factor in our model for, at the very least, allowing for intercepts to be estimated within-subject but across separate testing sessions. 

### Simulation-Based Power Analysis and Sample Size Calculations for fMRI Studies

As we can use fully-specified versions of our model to simulate data directly from the priors, we can use MCMC iterations as 'datasets' in the future to conduct power analyses --- that is, how large do effects in various ROIs have to be for us to reliably detect them with our model? Or, how large do within-subject or between-subjects differences in activity in an ROI have to be to detect them in a similar model with added regression parameters? Or, given an effect size, how many subjects in the dataset do we need to reliably find it? Such an applicaiton should be fairly straightforward in the future, as we used one iteration here to simulate data. We could either fix hyperparameters, or in a more Bayesian sense, center hyperparameter priors at our hypothesized effect size and draw samples from distributions. 


### Fully Bayesian fMRI Inference from Raw Data to Population Estimates

While we have mostly focused on the population-level inferences in this project, there are also substantial aspects of fMRI processing pipeline for getting voxel-level estimates at the subject level that could benefit from Bayesian hierarchical modeling. In particular, because estimates at each voxel at the subject level are estimated predictors in a regression model based on each voxel's timeseries over the course of the scan, multilevel modeling of these timeseries data could help considerably in estimating activity more precisely for each voxel for each subject. This approach has some significant challenges in tractability however. Such a fully specified model would have at least several parameters for each voxel (and several hundred datapoints in the timeseries per voxel per subject), and with at least several hundred thousand parameters, this model would take an extremely large amount of computational power and time to fit. While this approach seems a valuable one, this presents a significant challenge, so we may have to devise methods of precomuting values for priors and hyperparameters for making this approach more computationally efficient. If possible though, this method of fully Bayesian inference could help substantially in regularizing noisy estimates at the subject level. We hope to continue pursing this possibility especially with the goal of improving the efficiency of fMRI analyses for answering questions in neuroscience and psychology. 


# References

Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & Munaf??, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365???376. https://doi.org/10.1038/nrn3475
Chen, G., Xiao, Y., Taylor, P. A., Riggins, T., Geng, F., Redcay, E., & Cox, R. W. (2017). Handling Multiplicity in Neuroimaging through Bayesian Lenses with Hierarchical Modeling. BioRxiv, 238998. https://doi.org/10.1101/238998
Gee, D. G., Humphreys, K. L., Flannery, J., Goff, B., Telzer, E. H., Shapiro, M., ??? Tottenham, N. (2013). A Developmental Shift from Positive to Negative Connectivity in Human Amygdala???Prefrontal Circuitry. Journal of Neuroscience, 33(10), 4584???4593. https://doi.org/10.1523/JNEUROSCI.3446-12.2013
Gelman, A., Hill, J., & Yajima, M. (2012). Why We (Usually) Don???t Have to Worry About Multiple Comparisons. Journal of Research on Educational Effectiveness, 5(2), 189???211. https://doi.org/10.1080/19345747.2011.618213
Genovese, C. R., Lazar, N. A., & Nichols, T. (2002). Thresholding of Statistical Maps in Functional Neuroimaging Using the False Discovery Rate. NeuroImage, 15(4), 870???878. https://doi.org/10.1006/nimg.2001.1037
Geuter, S., Qi, G., Welsh, R. C., Wager, T. D., & Lindquist, M. A. (2018). Effect Size and Power in fMRI Group Analysis. BioRxiv, 295048. https://doi.org/10.1101/295048
Worsley, K. J. (2005). Spatial smoothing of autocorrelations to control the degrees of freedom in fMRI analysis. NeuroImage, 26(2), 635???641. https://doi.org/10.1016/j.neuroimage.2005.02.007
